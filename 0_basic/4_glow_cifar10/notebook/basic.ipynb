{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import scipy.linalg\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "''' load MNIST database '''\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "dataset_path = '../mnist_dataset'\n",
    "\n",
    "train_dataset = CIFAR10(dataset_path, transform=transform, train=True, download=True)\n",
    "valid_dataset = CIFAR10(dataset_path, transform=transform, train=False, download=True)\n",
    "test_dataset = CIFAR10(dataset_path, transform=transform, train=False, download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' load MNIST dataset by using dataloader'''\n",
    "batch_size = 64\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                         batch_size=1,\n",
    "                         shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782\n",
      "torch.Size([64, 3, 32, 32]) torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "''' test data loader'''\n",
    "print(len(train_loader))\n",
    "for batch_idx, (image, label) in enumerate(train_loader):\n",
    "    if (batch_idx + 1) % 100 == 0:\n",
    "        print(image.shape, label.shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  Energy: original: 195.356, reconstructed: 195.356, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 1.0000\n",
      ">>  Energy: original: 195.356, reconstructed: 195.356, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 1.0000\n"
     ]
    }
   ],
   "source": [
    "''' invertible 1x1 convolution '''\n",
    "def test_diff(x, x_hat):\n",
    "    err = x_hat - x\n",
    "\n",
    "    erg_x = torch.sum(x ** 2)\n",
    "    erg_x_hat = torch.sum(x_hat ** 2)\n",
    "    erg_err = torch.sum(err ** 2)\n",
    "    corr = F.conv2d(x, x_hat).squeeze() / torch.sqrt(erg_x * erg_x_hat)\n",
    "    corr = torch.mean(corr)\n",
    "    print('>>  Energy: original: {:.3f}, reconstructed: {:.3f}, error: {:.3f}'.format(\n",
    "            erg_x, erg_x_hat, erg_err))\n",
    "    print('>>  Correlation btw original and reconstructed image: {:.4f}'.format(\n",
    "            corr))\n",
    "\n",
    "\n",
    "''' Test implementation ''' \n",
    "n_chn = 3\n",
    "h = 8\n",
    "w = 8\n",
    "\n",
    "# Sample a random orthoronal matrix to initialize weights\n",
    "w_init = torch.qr(torch.randn(n_chn, n_chn))[0]\n",
    "\n",
    "# forward\n",
    "conv1x1 = nn.Conv2d(n_chn, n_chn, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "conv1x1.weight.data = w_init.view(n_chn, n_chn, 1, 1) # outchn x inchn x kerkel_size\n",
    "\n",
    "x = torch.randn(1, n_chn, w, h)\n",
    "y = conv1x1(x)\n",
    "\n",
    "# backward (=inference)\n",
    "w_inv_init = torch.inverse(w_init)\n",
    "conv1x1_inv = nn.Conv2d(n_chn, n_chn, kernel_size=1, stride=1, padding=0, bias=False)\n",
    "conv1x1_inv.weight.data = w_inv_init.view(n_chn, n_chn, 1, 1) # outchn x inchn x kerkel_size\n",
    "\n",
    "x_hat = conv1x1_inv(conv1x1(x))\n",
    "\n",
    "\n",
    "test_diff(x, x_hat)\n",
    "\n",
    "''' Class implementation '''\n",
    "class Invertible1x1conv(nn.Module):\n",
    "    def __init__(self, n_chn):\n",
    "        super(Invertible1x1conv, self).__init__()\n",
    "        self.n_chn = n_chn\n",
    "        \n",
    "        # sample a random orthoronal matrix to initialize weights\n",
    "        self.weight = torch.qr(torch.randn(n_chn, n_chn))[0]\n",
    "        \n",
    "    def forward(self, x, logdet=None, reverse=False):\n",
    "        # compute log determinant\n",
    "        _, logabsdet = torch.slogdet(self.weight)\n",
    "        dlogdet = x.size(-2) * x.size(-1) * logabsdet / x.size(0)\n",
    "\n",
    "        if not reverse: \n",
    "            # forward pass\n",
    "            w = self.weight\n",
    "            w = w.view(self.n_chn, self.n_chn, 1, 1)\n",
    "            y = F.conv2d(x, w)\n",
    "#             print('conv1x1: {:.3f}'.format(dlogdet))\n",
    "            if logdet is not None:\n",
    "                logdet = logdet + dlogdet\n",
    "            \n",
    "            return y, logdet\n",
    "        else:\n",
    "            # backward pass\n",
    "            w_inv = torch.inverse(self.weight)\n",
    "            w_inv = w_inv.view(self.n_chn, self.n_chn, 1, 1)\n",
    "            y = F.conv2d(x, w_inv)\n",
    "            \n",
    "            if logdet is not None:\n",
    "                logdet = logdet - dlogdet\n",
    "            \n",
    "            return y, logdet\n",
    "        \n",
    "invconv1x1 = Invertible1x1conv(n_chn)\n",
    "y, _ = invconv1x1(x, reverse=False)\n",
    "x_hat, _ = invconv1x1(y, reverse=True)\n",
    "\n",
    "test_diff(x, x_hat)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  Energy: original: 507.903, reconstructed: 507.903, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 1.0000\n",
      ">>  Energy: original: 507.903, reconstructed: 507.903, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 1.0000\n"
     ]
    }
   ],
   "source": [
    "''' ActNorm '''\n",
    "''' Test implementation '''\n",
    "n_chn = 8\n",
    "h = 8\n",
    "w = 8\n",
    "\n",
    "x = torch.randn(1, n_chn, w, h)\n",
    "\n",
    "# Initialize parameters\n",
    "logs = nn.Parameter(torch.zeros((1, n_chn, 1, 1), dtype=torch.float, requires_grad=True))\n",
    "b = nn.Parameter(torch.zeros((1, n_chn, 1, 1), dtype=torch.float, requires_grad=True))\n",
    "\n",
    "dlogdet = h * w * torch.sum(torch.abs(logs))\n",
    "\n",
    "# Forward\n",
    "y = x * torch.exp(logs) + b\n",
    "\n",
    "# Backward\n",
    "x_hat = (y - b) * torch.exp(-logs)\n",
    "\n",
    "test_diff(x, x_hat)\n",
    "\n",
    "''' Class implementation '''\n",
    "class ActNorm(nn.Module):\n",
    "    def __init__(self, n_chn):\n",
    "        super(ActNorm, self).__init__()\n",
    "        self.logs = nn.Parameter(torch.zeros((1, n_chn, 1, 1), dtype=torch.float, requires_grad=True))\n",
    "        self.b = nn.Parameter(torch.zeros((1, n_chn, 1, 1), dtype=torch.float, requires_grad=True))\n",
    "    \n",
    "    def forward(self, x, logdet=None, reverse=False):\n",
    "        dlogdet = x.size(-2) * x.size(-1) * torch.sum(torch.abs(self.logs)) / x.size(0)\n",
    "        if not reverse: \n",
    "#             print('actnorm: {:.3f}'.format(dlogdet))\n",
    "            \n",
    "            # forward pass\n",
    "            y = x * torch.exp(self.logs) + self.b\n",
    "            \n",
    "            if logdet is not None:\n",
    "                logdet = logdet + dlogdet\n",
    "            \n",
    "            return y, logdet\n",
    "        else:\n",
    "            # backward pass\n",
    "            y = (x - self.b) * torch.exp(self.logs)\n",
    "            \n",
    "            if logdet is not None:\n",
    "                logdet = logdet - dlogdet\n",
    "            \n",
    "            return y, logdet\n",
    "\n",
    "actnorm = ActNorm(n_chn)\n",
    "y, _ = actnorm(x, reverse=False)\n",
    "x_hat, _ = actnorm(y, reverse=True)\n",
    "test_diff(x, x_hat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>  Energy: original: 497.367, reconstructed: 497.367, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 1.0000\n",
      "torch.Size([1, 4, 8, 8])\n",
      "torch.Size([1, 4, 8, 8])\n",
      ">>  Energy: original: 497.367, reconstructed: 497.367, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 1.0000\n"
     ]
    }
   ],
   "source": [
    "''' Affine coupling layer '''\n",
    "class AffineNN(nn.Module):\n",
    "    def __init__(self, n_chn):\n",
    "        super(AffineNN, self).__init__()\n",
    "        self.n_chn = n_chn\n",
    "        self.n_half = n_chn // 2\n",
    "        self.conv1 = nn.Conv2d(n_chn, 2 * n_chn, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(2 * n_chn, 2 * n_chn, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        res = x\n",
    "        x = self.conv2(x)\n",
    "        x = x + res\n",
    "        return x[:, :self.n_chn], x[:, self.n_chn:]\n",
    "    \n",
    "''' Test implementation '''\n",
    "n_chn = 8\n",
    "h = 8\n",
    "w = 8\n",
    "\n",
    "x = torch.randn(1, n_chn, w, h)\n",
    "\n",
    "#### Forward\n",
    "# Split tensor\n",
    "n_half = n_chn // 2\n",
    "xa, xb = x[:, :n_half], x[:, n_half:]\n",
    "\n",
    "# Affine transform\n",
    "affineNN = AffineNN(n_half)\n",
    "logs, b = affineNN(xa)\n",
    "xb2 = xb * torch.exp(logs) + b\n",
    "\n",
    "dlogdet = torch.sum(torch.abs(logs))\n",
    "\n",
    "# Concatenate\n",
    "y = torch.cat((xa, xb2), dim=1)\n",
    "\n",
    "#### Backward\n",
    "# Split tensor\n",
    "xa, xb = y[:, :n_half], y[:, n_half:]\n",
    "# Affine transform\n",
    "logs, b = affineNN(xa)\n",
    "xb2 = xb * torch.exp(logs) + b\n",
    "xb2 = (xb - b) * torch.exp(-logs)\n",
    "# Concatenate\n",
    "x_hat = torch.cat((xa, xb2), dim=1)\n",
    "\n",
    "test_diff(x, x_hat)\n",
    "\n",
    "\n",
    "''' Class implementation '''\n",
    "class AffineCoupling(nn.Module):\n",
    "    def __init__(self, n_chn):\n",
    "        super(AffineCoupling, self).__init__()\n",
    "        self.n_chn = n_chn\n",
    "        self.n_half = n_chn // 2\n",
    "        self.transform = AffineNN(n_chn // 2)\n",
    "    \n",
    "    def forward(self, x, logdet=None, reverse=False):\n",
    "        # split\n",
    "        xa, xb = x[:, :self.n_half], x[:, self.n_half:]\n",
    "        # affine transform\n",
    "        logs, b = self.transform(xa)\n",
    "        dlogdet = torch.sum(torch.abs(logs)) / x.size(0)\n",
    "        print(torch.abs(logs).size())\n",
    "        if not reverse: \n",
    "#             print('affine coupling: {:.3f}'.format(dlogdet))\n",
    "\n",
    "            # forward pass\n",
    "            xb2 = xb * torch.exp(logs) + b\n",
    "            \n",
    "            # concatenate\n",
    "            y = torch.cat((xa, xb2), dim=1)\n",
    "            \n",
    "            if logdet is not None:\n",
    "                logdet = logdet + dlogdet\n",
    "            \n",
    "            return y, logdet\n",
    "        else:\n",
    "            # backward pass\n",
    "            xb2 = (xb - b) * torch.exp(-logs)\n",
    "            \n",
    "            # concatenate\n",
    "            y = torch.cat((xa, xb2), dim=1)\n",
    "            \n",
    "            if logdet is not None:\n",
    "                logdet = logdet - dlogdet\n",
    "            \n",
    "            return y, logdet\n",
    "\n",
    "affcoupling = AffineCoupling(n_chn)\n",
    "y, _ = affcoupling(x, reverse=False)\n",
    "x_hat, _ = affcoupling(y, reverse=True)\n",
    "test_diff(x, x_hat)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 4, 4])\n",
      "torch.Size([1, 12, 2, 2])\n",
      "torch.Size([1, 48, 1, 1])\n",
      "torch.Size([1, 12, 2, 2])\n",
      "torch.Size([1, 3, 4, 4])\n",
      ">>  Energy: original: 48.911, reconstructed: 48.911, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 1.0000\n",
      ">>  Energy: original: 48.911, reconstructed: 48.911, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 1.0000\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Squeezing and inverse squeezing operation \"\"\"\n",
    "''' test implementation'''\n",
    "x = torch.randn(1, 3, 4, 4)\n",
    "x_ori = x\n",
    "n_squeeze = 2\n",
    "print(x.size())\n",
    "for i in range(n_squeeze):\n",
    "    n_chn, n_hor, n_ver = x.size()[1:]\n",
    "    x = x.view(-1, n_chn * 4, n_hor // 2, n_ver // 2)\n",
    "    print(x.size())\n",
    "\n",
    "for i in range(n_squeeze):\n",
    "    n_chn, n_hor, n_ver = x.size()[1:]\n",
    "    x = x.view(-1, n_chn // 4, n_hor * 2, n_ver * 2)\n",
    "    print(x.size())\n",
    "\n",
    "test_diff(x, x_ori)\n",
    "\n",
    "''' class implementation '''\n",
    "class Squeeze2x2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Squeeze2x2, self).__init__()\n",
    "        \n",
    "    def forward(self, x, reverse=False):\n",
    "        n_chn, dim_hor, dim_ver = x.size()[1:]\n",
    "        \n",
    "        if not reverse:\n",
    "            x = x.view(-1, n_chn * 4, dim_hor // 2, dim_ver // 2)\n",
    "            return x\n",
    "        else:\n",
    "            x = x.view(-1, n_chn // 4, dim_hor * 2, dim_ver * 2)\n",
    "            return x\n",
    "\n",
    "\n",
    "squeeze2x2 = Squeeze2x2()\n",
    "for i in range(n_squeeze):\n",
    "    x = squeeze2x2(x, reverse=False)\n",
    "\n",
    "for i in range(n_squeeze):\n",
    "    x = squeeze2x2(x, reverse=True)\n",
    "\n",
    "test_diff(x, x_ori)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 8, 8])\n",
      "torch.Size([1, 4, 8, 8])\n",
      ">>  Energy: original: 495.091, reconstructed: 495.091, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 1.0000\n",
      "torch.Size([1, 4, 8, 8])\n",
      "torch.Size([1, 4, 8, 8])\n",
      ">>  Energy: original: 495.091, reconstructed: 495.091, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 1.0000\n"
     ]
    }
   ],
   "source": [
    "''' One step of flow '''\n",
    "#### actorm -> inv1x1conv -> affine coupling\n",
    "''' Test implementation '''\n",
    "n_chn = 8\n",
    "h = 8\n",
    "w = 8\n",
    "x = torch.randn(1, n_chn, w, h)\n",
    "logdet = 0.\n",
    "\n",
    "actnorm = ActNorm(n_chn)\n",
    "inv1x1conv = Invertible1x1conv(n_chn)\n",
    "affcoupling = AffineCoupling(n_chn)\n",
    "\n",
    "# Forward pass\n",
    "y, logdet = actnorm(x, logdet=logdet, reverse=False)\n",
    "y, logdet = inv1x1conv(y, logdet=logdet, reverse=False)\n",
    "y, logdet = affcoupling(y, logdet=logdet, reverse=False)\n",
    "\n",
    "# Backward pass\n",
    "x_hat, logdet = affcoupling(y, logdet=logdet, reverse=True)\n",
    "x_hat, logdet = inv1x1conv(x_hat, logdet=logdet, reverse=True)\n",
    "x_hat, logdet = actnorm(x_hat, logdet=logdet, reverse=True)\n",
    "test_diff(x, x_hat)\n",
    "\n",
    "''' Class implementation '''\n",
    "class FlowStep(nn.Module):\n",
    "    def __init__(self, n_chn):\n",
    "        super(FlowStep, self).__init__()\n",
    "        self.n_chn = n_chn\n",
    "        self.actnorm = ActNorm(n_chn)\n",
    "        self.inv1x1conv = Invertible1x1conv(n_chn)\n",
    "        self.affcoupling = AffineCoupling(n_chn)\n",
    "    \n",
    "    def forward(self, x, logdet=None, reverse=False):\n",
    "        if not reverse:\n",
    "            # Forward pass\n",
    "            y, logdet = self.actnorm(x, logdet=logdet, reverse=False)\n",
    "            y, logdet = self.inv1x1conv(y, logdet=logdet, reverse=False)\n",
    "            y, logdet = self.affcoupling(y, logdet=logdet, reverse=False)\n",
    "            return y, logdet\n",
    "        \n",
    "        else:\n",
    "            # Backward pass\n",
    "            y, logdet = self.affcoupling(x, logdet=logdet, reverse=True)\n",
    "            y, logdet = self.inv1x1conv(y, logdet=logdet, reverse=True)\n",
    "            y, logdet = self.actnorm(y, logdet=logdet, reverse=True)\n",
    "            return y, logdet\n",
    "        \n",
    "flowstep = FlowStep(n_chn)\n",
    "y, _ = flowstep(x, reverse=False)\n",
    "x_hat, _ = flowstep(y, reverse=True)\n",
    "test_diff(x, x_hat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Glow(\n",
      "  (squeeze2x2): Squeeze2x2()\n",
      "  (layers): ModuleList(\n",
      "    (0): FlowStep(\n",
      "      (actnorm): ActNorm()\n",
      "      (inv1x1conv): Invertible1x1conv()\n",
      "      (affcoupling): AffineCoupling(\n",
      "        (transform): AffineNN(\n",
      "          (conv1): Conv2d(6, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(12, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): FlowStep(\n",
      "      (actnorm): ActNorm()\n",
      "      (inv1x1conv): Invertible1x1conv()\n",
      "      (affcoupling): AffineCoupling(\n",
      "        (transform): AffineNN(\n",
      "          (conv1): Conv2d(24, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(48, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): FlowStep(\n",
      "      (actnorm): ActNorm()\n",
      "      (inv1x1conv): Invertible1x1conv()\n",
      "      (affcoupling): AffineCoupling(\n",
      "        (transform): AffineNN(\n",
      "          (conv1): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (3): FlowStep(\n",
      "      (actnorm): ActNorm()\n",
      "      (inv1x1conv): Invertible1x1conv()\n",
      "      (affcoupling): AffineCoupling(\n",
      "        (transform): AffineNN(\n",
      "          (conv1): Conv2d(96, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "          (conv2): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      ">>  Energy: original: 51188.836, reconstructed: 51188.828, error: 0.000\n",
      ">>  Correlation btw original and reconstructed image: 0.0006\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' Entire network '''\n",
    "class Glow(nn.Module):\n",
    "    def __init__(self, n_chn, n_flow, squeeze_layer=[0, 1, 2]):\n",
    "        super(Glow, self).__init__()\n",
    "        self.n_chn = n_chn\n",
    "        self.n_flow = n_flow\n",
    "        self.squeeze_layer = squeeze_layer\n",
    "        \n",
    "        self.squeeze2x2 = Squeeze2x2()\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i_flow in range(n_flow):\n",
    "            if i_flow in squeeze_layer:\n",
    "                n_chn = n_chn * 4\n",
    "            self.layers.append(FlowStep(n_chn))\n",
    "            \n",
    "    def forward(self, x, logdet=None, reverse=False):\n",
    "        if not reverse:\n",
    "            for i_flow in range(self.n_flow):\n",
    "                if i_flow in self.squeeze_layer:\n",
    "                    x = self.squeeze2x2(x, reverse=False)\n",
    "                x, logdet = self.layers[i_flow](x, logdet=logdet, reverse=False)\n",
    "            return x, logdet\n",
    "        else:\n",
    "            for i_flow in reversed(range(self.n_flow)):\n",
    "                x, logdet = self.layers[i_flow](x, logdet=logdet, reverse=True)\n",
    "                if i_flow in self.squeeze_layer:\n",
    "                    x = self.squeeze2x2(x, reverse=True)\n",
    "            return x, logdet\n",
    "\n",
    "model = Glow(n_chn=3, n_flow=4, squeeze_layer=[0, 1, 2])\n",
    "print(model)\n",
    "\n",
    "# Test invertibility\n",
    "x, _ = iter(train_loader).next()\n",
    "y, logdet = model(x, logdet=0., reverse=False)\n",
    "x_hat, logdet = model(y, logdet=logdet, reverse=True)\n",
    "test_diff(x, x_hat)\n",
    "\n",
    "# Plot\n",
    "x_tmp, x_hat_tmp = x[0], x_hat[0]\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(x_tmp.permute(1, 2, 0))\n",
    "plt.clim(-1, 1)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(x_hat_tmp.permute(1, 2, 0).detach().numpy())\n",
    "plt.clim(-1, 1)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "''' Training criteria and optimizer definition '''\n",
    "z, logdet = model(x, logdet=0., reverse=False)\n",
    "\n",
    "class GlowLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GlowLoss, self).__init__()\n",
    "        \n",
    "    def forward(self, z, logdet):  \n",
    "        log_prior = -torch.sum(z ** 2) / 2\n",
    "        nll = -(log_prior + logdet)\n",
    "        return nll\n",
    "\n",
    "criterion = GlowLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "print(criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTrain loss: 145060672.000000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-a18ba9630e54>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#         print(loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "''' Train network '''\n",
    "num_epochs = 6\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss_avg = 0.\n",
    "    for image, label in train_loader:\n",
    "        z, logdet = model(image, logdet=0., reverse=False)\n",
    "        loss = criterion(z, logdet)\n",
    "        loss.backward()\n",
    "#         print(loss)\n",
    "        optimizer.step()\n",
    "        loss_avg += loss / len(train_loader)\n",
    "        \n",
    "    print('Epoch: {:} \\tTrain loss: {:.6f}'.format(\n",
    "        epoch+1, loss_avg))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actnorm: 0.000\n",
      "conv1x1: -0.000\n",
      "affine coupling: 53.506\n",
      "actnorm: 0.000\n",
      "conv1x1: -0.000\n",
      "affine coupling: 137.329\n",
      "actnorm: 0.000\n",
      "conv1x1: -0.000\n",
      "affine coupling: 128.510\n",
      "actnorm: 0.000\n",
      "conv1x1: -0.000\n",
      "affine coupling: 150.225\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for ** or pow(): 'tuple' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-54ed02451a22>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# Evaluate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mmodel_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-9046a3a72279>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, z, logdet)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mlog_prior\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mnll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_prior\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlogdet\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for ** or pow(): 'tuple' and 'int'"
     ]
    }
   ],
   "source": [
    "''' Test model '''\n",
    "test_loss = 0.\n",
    "accuracy_total = 0\n",
    "model.eval()\n",
    "for image, label in test_loader:\n",
    "    # Evaluate loss\n",
    "    model_out = model(image)\n",
    "    loss = criterion(model_out, label)\n",
    "    test_loss += loss / len(test_loader)\n",
    "    \n",
    "    # Evaluate classification accuracy\n",
    "    _, pred = torch.max(model_out, dim=1)\n",
    "    accuracy = torch.sum((pred == label).float())\n",
    "    accuracy_total += accuracy / len(test_loader)\n",
    "    \n",
    "print('Test set: Accuracy: {:.2f}, Loss: {:.6f}'.format(\n",
    "    accuracy_total, test_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
